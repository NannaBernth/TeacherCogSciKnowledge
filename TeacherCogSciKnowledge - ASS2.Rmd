---
title: "Assignment 2"
author: "Nanna Bernth"
date: "19 feb 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

setwd("C:/Users/nanna/OneDrive - Aarhus universitet/4. Semester/Experimental Methods 4/Assignment 2/TeacherCogSciKnowledge")

library(pacman)
library(devtools)
devtools::install_github("rmcelreath/rethinking")
library(rethinking)

```

## In this assignment we learn how to assess rates from a binomial distribution, using the case of assessing your teachers' knowledge of CogSci

### First part

You want to assess your teachers' knowledge of cognitive science. "These guys are a bunch of drama(turgist) queens, mindless philosophers, chattering communication people and Russian spies. Do they really know CogSci?", you think.

To keep things simple (your teachers should not be faced with too complicated things):

- You created a pool of equally challenging questions on CogSci
- Each question can be answered correctly or not (we don't allow partially correct answers, to make our life simpler).
- Knowledge of CogSci can be measured on a scale from 0 (negative knowledge, all answers wrong) through 0.5 (random chance) to 1 (awesome CogSci superpowers)

This is the data:
- Riccardo: 3 correct answers out of 6 questions
- Kristian: 2 correct answers out of 2 questions (then he gets bored)
- Josh: 160 correct answers out of 198 questions (Josh never gets bored)
- Mikkel: 66 correct answers out of 132 questions

Questions:

###1. What's Riccardo's estimated knowledge of CogSci? What is the probability he knows more than chance (0.5) [try figuring this out. if you can't peek into chapters 3.1 and 3.2 and/or the slides]?

- First implement a grid approximation (hint check paragraph 2.4.1!) with a uniform prior, calculate the posterior and plot the results

- Then implement a quadratic approximation (hint check paragraph 2.4.2!).

- N.B. for the rest of the exercise just keep using the grid approximation (we'll move to quadratic approximations in two classes)

```{r + grid approximation, include=FALSE}

#define grid
p_grid <- seq(from= 0, to = 1, length.out = 20)

#define prior
prior <- rep(1,6)

#compute likelihood at each value in grid
likelihood <- dbinom(3 , size = 6, prob = p_grid)

#compute product of likelihood and prior
unstd.posterior <- likelihood * prior

#standardise the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

#display distribution
plot(p_grid, posterior, type ="b", xlab ="probability of correct answer", ylab = "posterior probability")

```



```{r + Quadratic, include=FALSE}

globe.qa <- map(
  alist(
    w ~ dbinom(6,p), #binomial likelihood
    p ~ dunif(0,1) #uniform prior
  ), 
  data = list(w=3)  )

#display summary of quadratic approximation
precis(globe.qa)

```

```{r, chance of >.5, include=FALSE}
#Adding up the posterior probablity of 

sum ( posterior[p_grid > 0.5])

```

The density plot shows Riccardos estimated knowledge of CogSci. There is no probability that Riccardo knows nothing and likewise no probability that he knows everything. The distribution is centered around chance level (prob = 0.5) but is very wide and hence Riccardos estimated knowledge is most likely to be close to chance level (the peak), but witht the current evidence we are quite uncertain and the probabilities are high at a large interval.  

The quadratic approximation elaborates on this, assuming the posterior is gaussian, is it maximized at 0.5, and its standard deviation is 0.2. 

According to the data, there is a 0.5 probability that Riccardo knows more than chance. 



###2. Estimate all the teachers' knowledge of CogSci. Who's best? Use grid approximation. Comment on the posteriors of Riccardo and Mikkel.

2a. Produce plots of the prior, and posterior for each teacher.

```{r + prior uniform}

#function to grid approx. 
likelihood_p <- function(p, count, size) {
  return(dbinom(prob = p, x = count, size = size))
} # likelihood of p given the data


p_grid <- seq(0,1, length.out = 200)
prior_uni <- rep(1,200)

right_riccardo <- 3 
right_tylle <- 2
right_josh <- 160
right_mikkel <- 66


total_riccardo <- 6
total_tylle <- 2
total_josh <- 198
total_mikkel <- 132

#Riccardo
riccardo <- likelihood_p(p_grid, 
                      count = right_riccardo,
                      size = total_riccardo) * prior_uni

riccardo_std <- riccardo/sum(riccardo)

#Tylle
tylle <- likelihood_p(p_grid, 
                      count = right_tylle,
                      size = total_tylle) * prior_uni

tylle_std <- tylle/sum(tylle)


#Josh
josh <- likelihood_p(p_grid, 
                      count = right_josh,
                      size = total_josh) * prior_uni

josh_std <- josh/sum(josh)


#Mikkel
mikkel <- likelihood_p(p_grid, 
                      count = right_mikkel,
                      size = total_mikkel) * prior_uni

mikkel_std <- mikkel/sum(mikkel)


##Bad way to show the data, but still let's see the prob. of >.5 knowledge
sum ( riccardo_std[p_grid > 0.5])
sum ( tylle_std[p_grid > 0.5])
sum ( josh_std[p_grid > 0.5])
sum ( mikkel_std[p_grid > 0.5])


p_grid[ which.max(riccardo_std) ]
p_grid[ which.max(tylle_std) ]
p_grid[ which.max(mikkel_std) ]
p_grid[ which.max(josh_std) ]


```

```{r + uniform plots}
#Plots

plot(p_grid, prior_uni , type = "l", xlab ="probability of correct answer (prior) ", ylab = "posterior probability")

plot(p_grid, riccardo_std, type = "l", xlab ="probability of correct answer (Riccardo)", ylab = "posterior probability")
plot(p_grid, tylle_std, type = "l",xlab ="probability of correct answer (Tylle)", ylab = "posterior probability")
plot(p_grid, josh_std, type = "l", xlab ="probability of correct answer (Josh)", ylab = "posterior probability")
plot(p_grid, mikkel_std, type = "l", xlab ="probability of correct answer (Mikkel)", ylab = "posterior probability")



#One plot for fun 
plot <- ggplot() +
  aes(x = p_grid, y = mikkel_std) +
  geom_line(aes(col = 'red'), size = 1) + ylab('posterior probability') + xlab('Probability of correct answer')
plot <- plot + geom_line(aes(y= riccardo_std), colour="green", size = 1)
plot <- plot + geom_line(aes(y= tylle_std), colour="orange", size = 1)
plot <- plot + geom_line(aes(y= josh_std), colour="blue", size = 1) + theme_classic()
#plot <- plot + geom_line(aes(y= prior_uni), colour="black", size = 1) + 
plot
```


First there is a plot showing the flat/uniform prior. 

When looking at the plots for each teacher's posterior distribution, we would say that it appears Kristian is the teacher with most CogSci knowledge. Witht the current evidence, he is most likely to have "awesome knowledge" and less likely to have anything under. However, the amount of evidence emobodied in the plausibilities is not very large and the "area of uncertainty" cover the whole x-axis. 
The maximun height of the curve increases with each sample and the "narrowness" increases. This is clear as we can see in the plot of Josh's CogSci knowledge, the probability that he is a very clever teacher is high (above chance) and the curvature is very steap, hence, with the large amount of data, we are more confident that Josh' CogSci knowledge will fall within a much more narrow area than the other teachers.

Looking at Mikkel's and Riccardo's plot we can really see how the amount of data affects the distrubution. Both teachers have answered half the questions correct and peak at 0.5, however Mikkel's curve is much more narrow and concentrates on a muss more dense area - there is for instance no probability that he will havve a knowledge of either 0.3 or 0.7, whereas Riccado's curve (as already mentioned) is much more broad and show a higher probability that Mikkel to have a great deal more og a great deal less knowledge than the chance level.    



3. Change the prior. Given your teachers have all CogSci jobs, you should start with a higher appreciation of their knowledge: the prior is a normal distribution with a mean of 0.8 and a standard deviation of 0.2. Do the results change (and if so how)?
3a. Produce plots of the prior and posterior for each teacher.

```{r + Prior = 0.8, 0.2}

prior_p <- dnorm(p_grid, 0.8, 0.2)

plot(p_grid, prior_p , type = "l", xlab ="probability of correct answer (prior) ", ylab = "posterior probability")

#Riccardo
riccardo <- likelihood_p(p_grid, 
                      count = right_riccardo,
                      size = total_riccardo) * prior_p

riccardo_std_2 <- riccardo/sum(riccardo)

#Tylle
tylle <- likelihood_p(p_grid, 
                      count = right_tylle,
                      size = total_tylle) * prior_p

tylle_std_2 <- tylle/sum(tylle)


#Josh
josh <- likelihood_p(p_grid, 
                      count = right_josh,
                      size = total_josh) * prior_p

josh_std_2 <- josh/sum(josh)


#Mikkel
mikkel <- likelihood_p(p_grid, 
                      count = right_mikkel,
                      size = total_mikkel) * prior_p

mikkel_std_2 <- mikkel/sum(mikkel)



#Find peaks
p_grid[ which.max(riccardo_std_2) ]
p_grid[ which.max(tylle_std_2) ]
p_grid[ which.max(mikkel_std_2) ]
p_grid[ which.max(josh_std_2) ]


```


```{r + 0.8, 0.2 plots}

plot(p_grid, riccardo_std_2, type = "l", xlab ="probability of correct answer (Riccardo)", ylab = "posterior probability")

plot(p_grid, tylle_std_2, type = "l",xlab ="probability of correct answer (Tylle)", ylab = "posterior probability")

plot(p_grid, josh_std_2, type = "l", xlab ="probability of correct answer (Josh)", ylab = "posterior probability")

plot(p_grid, mikkel_std_2, type = "l", xlab ="probability of correct answer (Mikkel)", ylab = "posterior probability")


#For fun
plot <- ggplot() +
  aes(x = p_grid, y = mikkel_std_2 ) +
  geom_line(aes(col = 'red'))
plot <- plot + geom_line(aes(y= riccardo_std_2), colour="green")
plot <- plot + geom_line(aes(y= tylle_std_2), colour="orange")
plot <- plot + geom_line(aes(y= josh_std_2), colour="blue")
#plot <- plot + geom_line(aes(y= prior_p), colour="black")
plot
```

First, we can see how the prior i no longer flat but a distribution with the peak (mean) of 0.8 and a standard deviation of 0.2. 

As for the other plots, we almost see the same pattern as before. It is clear how a lot of evidence (data) is less affected by the change of the prior - hence, the distributions of Kristian and Riccardo changes quite a lot. All of the teacher's curves are pushed towards the mean of the prior.

Kristian would still be the teacher we would assume to have most knowledge of CogSci, however we see now how changing the prior, assuming a mean of 0.8 with a sd of 0.2 affects, for instance, the probability of having "awesome knowledge". Kristian's peak is no longer at one, but drawn closer towards the mean. 

Josh's curve already peaked around the new mean (0.8) and is not really affected by the new prior, however with the large amount of data for Josh, his distribution is more stable so another reasonable prior would not impact the distribution to a larger extent either. 
Riccardo's curve is more narrow now and pushed towards higher probability of having knowlegde above average. His plot, however, still yields great uncertainty of what his knowledge actually is compared to the more narrow curve of Mikkel and Josh. 

Mikkel is also pushed a bit towards the right, but the fact that we have more data to support his knowledge means that eventhough the peak of Mikkel and Riccardo previously was similar, Riccardo would now be assumed to have higher probability of more correct answers. 


4. You go back to your teachers and collect more data (multiply the previous numbers by 100). Calculate their knowledge with both a uniform prior and a normal prior with a mean of 0.8 and a standard deviation of 0.2. Do you still see a difference between the results? Why?

```{r + Additional Questions (uniform)}

add_right_riccardo <- 3 *100
add_right_tylle <- 2 *100
add_right_josh <- 160 *100
add_right_mikkel <- 66 *100
 
add_total_riccardo <- 6 *100
add_total_tylle <- 2 *100
add_total_josh <- 198 *100
add_total_mikkel <- 132 *100

#Riccardo
add_riccardo <- likelihood_p(p_grid, 
                      count = add_right_riccardo,
                      size = add_total_riccardo) * prior_uni

add_riccardo_std <- add_riccardo/sum(add_riccardo)

#Tylle
add_tylle <- likelihood_p(p_grid, 
                      count = add_right_tylle,
                      size = add_total_tylle) * prior_uni

add_tylle_std <- add_tylle/sum(add_tylle)


#Josh
add_josh <- likelihood_p(p_grid, 
                      count = add_right_josh,
                      size = add_total_josh) * prior_uni

add_josh_std <- add_josh/sum(add_josh)


#Mikkel
add_mikkel <- likelihood_p(p_grid, 
                      count = add_right_mikkel,
                      size = add_total_mikkel) * prior_uni

add_mikkel_std <- add_mikkel/sum(add_mikkel)



#Max point
p_grid[ which.max(add_riccardo_std) ]
p_grid[ which.max(add_tylle_std) ]
p_grid[ which.max(add_mikkel_std) ]
p_grid[ which.max(add_josh_std) ]


```

```{r + uniform + 100}
#Plots

plot(p_grid, prior_uni , type = "l", xlab ="probability of correct answer (prior) ", ylab = "posterior probability")

plot(p_grid, add_riccardo_std, type = "l", xlab ="probability of correct answer (Riccardo)", ylab = "posterior probability")

plot(p_grid, add_tylle_std, type = "l",xlab ="probability of correct answer (Tylle)", ylab = "posterior probability")
plot(p_grid, add_josh_std, type = "l", xlab ="probability of correct answer (Josh)", ylab = "posterior probability")
plot(p_grid, add_mikkel_std, type = "l", xlab ="probability of correct answer (Mikkel)", ylab = "posterior probability")

plot <- ggplot() +
  aes(x = p_grid, y = add_mikkel_std ) +
  geom_line(aes(col = 'red'))
plot <- plot + geom_line(aes(y= add_riccardo_std), colour="green")
plot <- plot + geom_line(aes(y= add_tylle_std), colour="orange")
plot <- plot + geom_line(aes(y= add_josh_std), colour="blue")
#plot <- plot + geom_line(aes(y= prior_uni), colour="black")
plot
```



```{r + Additional Questions (0.8, 0.2) }

prior_skep<- dnorm(p_grid, 0.5, 0.01)

#Riccardo
add_riccardo <- likelihood_p(p_grid, 
                      count = add_right_riccardo,
                      size = add_total_riccardo) * prior_p
add_riccardo_std_2 <- add_riccardo/sum(add_riccardo)


#Tylle
add_tylle <- likelihood_p(p_grid, 
                      count = add_right_tylle,
                      size = add_total_tylle) * prior_p

add_tylle_std_2 <- add_tylle/sum(add_tylle)


#Josh
add_josh <- likelihood_p(p_grid, 
                      count = add_right_josh,
                      size = add_total_josh) * prior_p

add_josh_std_2 <- add_josh/sum(add_josh)


#Mikkel
add_mikkel <- likelihood_p(p_grid, 
                      count = add_right_mikkel,
                      size = add_total_mikkel) * prior_p

add_mikkel_std_2 <- add_mikkel/sum(add_mikkel)


#Max point
p_grid[ which.max(add_riccardo_std_2) ]
p_grid[ which.max(add_tylle_std_2) ]
p_grid[ which.max(add_mikkel_std_2) ]
p_grid[ which.max(add_josh_std_2) ]


```

```{r + 0.8, 0.2 + 100 + Plots}

#Plots


plot(p_grid, prior_p , type = "l", xlab ="probability of correct answer (prior) ", ylab = "posterior probability", col = 'black')

plot(p_grid, add_riccardo_std_2, type = "l", xlab ="probability of correct answer (Riccardo)", ylab = "posterior probability")

plot(p_grid, add_tylle_std_2, type = "l", xlab ="probability of correct answer (Tylle)", ylab = "posterior probability")

plot(p_grid, add_josh_std_2, type = "l", xlab ="probability of correct answer (Josh)", ylab = "posterior probability")

plot(p_grid, add_mikkel_std_2, type = "l", xlab ="probability of correct answer (Mikkel)", ylab = "posterior probability")


plot <- ggplot() +
  aes(x = p_grid, y = add_mikkel_std_2 ) +
  geom_line(aes(col = 'red'))
plot <- plot + geom_line(aes(y= add_riccardo_std_2), colour="green")
plot <- plot + geom_line(aes(y= add_tylle_std_2), colour="orange")
plot <- plot + geom_line(aes(y= add_josh_std_2), colour="blue")
#plot <- plot + geom_line(aes(y= prior_p), colour="black")
plot

```


The difference between the posterior distributions are now minimal despite changing the prior from uniform to .8 (sd = 0.2). With a large amount of data the prior has less impact and does not affect the posterior distribution of the data as much. Moreover, we see how all the curves are narrowed down to less intervals on the x-axis, we are less uncertain of the intervals there the teachers' CogSci knowledge lies. 



5. Imagine you're a skeptic and think your teachers do not know anything about CogSci, given the content of their classes. How would you operationalize that belief?

A good way to be skeptical would be to set the mean the 0.5, because it is a y/n questionaire, so at the chance-level, and the set the standard deviation to 0.025 to minimize room for variance. 

In r code, this woud be:
prior_skeptial <- dnom(p_grid, 0.5, 0.025)




6. Optional question: Can you estimate the difference between Riccardo's estimated knowledge and that of each of the other teachers? Would you deem it credible (that is, would you believe that it is actually different)?

7. Bonus knowledge: all the stuff we have done can be implemented in a lme4-like fashion using the brms package. Here is an example.

```{r}
library(pacman)
p_load(DT)
p_load(brms)

d <- data.frame(
  Correct=c(3,2,160,66),
  Questions=c(6,2,198,132),
  Teacher=c("RF","KT","JS","MW"))

FlatModel <- brm(Correct|trials(Questions)~1, data = subset(d,Teacher=="RF"), prior = prior("uniform(0,1)", class = "Intercept"),family=binomial)
plot(FlatModel)

PositiveModel <- brm(Correct|trials(Questions)~1,data=subset(d,Teacher=="RF"),prior=prior("normal(0.8,0.2)", class = "Intercept"),family=binomial)
plot(PositiveModel)

SkepticalModel <- brm(Correct|trials(Questions)~1,data=subset(d,Teacher=="RF"),prior=prior("normal(0.5,0.01)", class = "Intercept"),family=binomial)
plot(SkepticalModel)
```

If you dare, try to tweak the data and model to test two hypotheses:
- Is Kristian different from Josh?
- Is Josh different from chance?



## Second part: Focusing on predictions

Last year you assessed the teachers (darned time runs quick!). Now you want to re-test them and assess whether your models are producing reliable predictions. In Methods 3 we learned how to do machine-learning style assessment of predictions (e.g. rmse on testing datasets). Bayesian stats makes things a bit more complicated. So we'll try out how that works. 
N.B. You can choose which prior to use for the analysis of last year's data.


Questions to be answered (but see guidance below):
###1- Write a paragraph discussing how assessment of prediction performance is different in Bayesian vs. frequentist models


In Experimental Methods 3, working under the frequentist paradigm in order to assess the predictive accuracy of our models, we used for instance use cross-validation to check a model’s RMSE, evaluate on the ROC or looking at residuals.  

In the Bayesian framework, the “output” is much more intuitive. We simply plot the posterior predictive distributions in which a need to assess the data’s corresponding assumptions is left open to interpretation. 

This level of Bayesian framework allows to make predictions based upon knowledge rather than inference; we allow data to shape the predictions and rather than being as dependent on whichever model we’re currently testing, enabling us to more easily inspect biases existing in our data and distinguish these from biases applied by a hypothetical model.
Similarly, the uncertainty of computed predictions are significantly more transparent with a Bayesian approach, whereas a frequentist approach relies on arbitrary statistical significance to deem it ‘certain’ or ‘uncertain’. 




###2- Provide at least one plot and one written line discussing prediction errors for each of the teachers.

This is the old data:
- Riccardo: 3 correct answers out of 6 questions
- Kristian: 2 correct answers out of 2 questions (then he gets bored)
- Josh: 160 correct answers out of 198 questions (Josh never gets bored)
- Mikkel: 66 correct answers out of 132 questions

This is the new data:
- Riccardo: 9 correct answers out of 10 questions (then he freaks out about teaching preparation and leaves)
- Kristian: 8 correct answers out of 12 questions
- Josh: 148 correct answers out of 172 questions (again, Josh never gets bored)
- Mikkel: 34 correct answers out of 65 questions


Guidance Tips

1. There are at least two ways of assessing predictions.
2. Last year's results are this year's expectations.
3. Are the parameter estimates changing? (way 1)
4. How does the new data look in last year's predictive posterior? (way 2)

```{r + way 2}

p_grid <- seq(0,1, length.out = 200)



#Samples
set.seed(69) # pre-determine the random number generator so we all get the same results

ric_samples = sample(p_grid, prob = riccardo_std_2, size = 1e4, replace = TRUE) # sample from the posterior
dens(ric_samples) 

set.seed(69)
tylle_samples = sample(p_grid, prob = tylle_std_2, size = 1e4, replace = TRUE) # sample from the posterior
dens(tylle_samples) 

set.seed(69)
josh_samples = sample(p_grid, prob = josh_std_2, size = 1e4, replace = TRUE) # sample from the posterior
dens(josh_samples) 

set.seed(69)
mikkel_samples = sample(p_grid, prob = mikkel_std_2, size = 1e4, replace = TRUE) # sample from the posterior
dens(mikkel_samples) 


#Posterior Predictive Distribution

ric_pre <- rbinom( 1e4 , size=10 , prob= ric_samples)
tylle_pre <- rbinom( 1e4 , size=12 , prob= tylle_samples)
josh_pre <- rbinom( 1e4 , size=172 , prob= josh_samples)
mikkel_pre <- rbinom( 1e4 , size=65 , prob= mikkel_samples)


#Histograms

simplehist(ric_pre, xlab = "Posterior Prediction (Riccardo)", col = blues9[-(1:3)])
        abline(v = 9.08, col = "black", lty = 3, lwd = 2)

simplehist(tylle_pre, xlab = "Posterior Prediction (Kristian)", col = blues9[-(1:3)])
        abline(v = 8.08, col = "black", lty = 3, lwd = 2)
           
simplehist(josh_pre, xlab = "Posterior Prediction (Josh)", col = blues9[-(1:3)])
        abline(v = 148.2, col = "black", lty = 3, lwd = 1.5)

simplehist(mikkel_pre, xlab = "Posterior Prediction (Mikkel)", col = blues9[-(1:3)])
        abline(v = 34.2, col = "black", lty = 3, lwd = 1.5)
        
        
simplehist(ric_pre - 10)

```


Riccardo: With the prior questionnaire data our estimates of Riccardo’s knowledge turn out to be lower than how he performs on the new questionnaire. Despite the fact, that his estimated knowledge is increased by our prior (0.8), he outperforms our model predictions. Riccardo did his homework. 

Tylle: With the prior questionnaire data our estimates of Kristian’s knowledge turn out to be higher than how he performs on the new questionnaire. With the new data, Kristan no longer achieves “awesome knowledge” - it turns out he may not be as perfect as we thought. 

Josh: With the prior questionnaire data our estimates of Josh’s knowledge turn out to be lower than how he performs on the new questionnaire. Josh is really keeping up on new articles. He keeps getting better and it pays off, however it makes our model a bad predictor of him! 

Mikkel: With the prior questionnaire data our estimates of Mikkel’s knowledge turn out to be just slightly off compared to how he performs on the new questionnaire. We would have predicted a correct amount of answers close to chance level (33 correct) and Mikkel got 34 correct. He might not be the “best” of the teachers, but you always know what you get. 


### Depending on time: Questions from the handbook
2H1. Suppose there are two species of panda bear. Both are equally common in the wild and live in the same places. They look exactly alike and eat the same food, and there is yet no genetic assay capable of telling them apart. They differ however in their family sizes. Species A gives birth to twins 10% of the time, otherwise birthing a single infant. Species B births twins 20% of the time, otherwise birthing singleton infants. Assume these numbers are known with certainty, from many years of field research.
Now suppose you are managing a captive panda breeding program. You have a new female panda of unknown species, and she has just given birth to twins. What is the probability that her next birth will also be twins?

2H2. Recall all the facts from the problem above. Now compute the probability that the panda we have is from species A, assuming we have observed only the first birth and that it was twins.

2H3. Continuing on from the previous problem, suppose the same panda mother has a second birth and that it is not twins, but a singleton infant. Compute the posterior probability that this panda is species A.

2H4. A common boast of Bayesian statisticians is that Bayesian inference makes it easy to use all of the data, even if the data are of different types. So suppose now that a veterinarian comes along who has a new genetic test that she claims can identify the species of our mother panda. But the test, like all tests, is imperfect. This is the information you have about the test:
- The probability it correctly identifies a species A panda is 0.8.
- The probability it correctly identifies a species B panda is 0.65.
The vet administers the test to your panda and tells you that the test is positive for species A. First ignore your previous information from the births and compute the posterior probability that your panda is species A. Then redo your calculation, now using the birth data as well.
